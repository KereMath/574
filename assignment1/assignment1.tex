\documentclass[11pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amsmath}

\geometry{top=2.5cm, bottom=2.5cm, left=3cm, right=3cm}
\onehalfspacing
\setlength{\parskip}{6pt}

\hypersetup{colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=black}

\titleformat{\section}{\normalsize\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}

\begin{document}

\begin{flushleft}
  Kerem Recep G\"{u}r \ $|$ \ 2448462 \hfill CENG 574 --- Article Review
\end{flushleft}
\rule{\linewidth}{0.4pt}

\vspace{0.5em}

\section{Summary of the Article}

The article by Li et al.\ (2022) investigates how big data analytics usage (BDU)
affects the quality of organizational decision making.
The authors build their argument on \textbf{dynamic capability theory}, which
divides organizational capabilities into two levels: low-order capabilities, which
are basic operational skills, and high-order capabilities, which are more advanced
and harder to imitate.
According to this framework, BDU is a low-order capability that helps firms
build data analytics capabilities (DAC)---a high-order capability---which in turn
leads to better decisions.

To test this argument, the authors collected survey data from 240 agricultural
firms in China and analyzed them using \textbf{PLS-SEM} (SmartPLS 3.3.2).
Three hypotheses were tested and all were supported:

\begin{itemize}[leftmargin=1.5em]
  \item \textbf{H1:} BDU has a direct positive effect on decision-making quality
        ($\beta = 0.223$, $p < 0.01$).
  \item \textbf{H2:} BDU significantly contributes to the development of DAC
        ($\beta = 0.623$, $p < 0.001$).
  \item \textbf{H3:} DAC has a positive effect on decision-making quality
        ($\beta = 0.394$, $p < 0.001$).
\end{itemize}

In addition, the mediation analysis showed that DAC acts as a
\textbf{partial mediator} between BDU and decision-making quality.
The indirect effect was statistically significant ($\text{effect} = 0.218$,
$p < 0.001$), meaning that part of the benefit of using big data tools reaches
decision quality through the organizational capabilities that are built along the way.

\section{Strengths of the Study}

\subsection{Bias Controls and Data Quality}

The study applies several important checks to make sure the survey data is
trustworthy.
To deal with \textbf{common method bias}---a common problem in studies where
the same person answers all questions---the authors used the marker variable
technique proposed by Lindell and Whitney (2001).
The average marker correlation was very low ($r_{M} = 0.014$), and the
differences between adjusted and unadjusted path estimates were small
($\Delta r \leq 0.010$), which suggests that this type of bias was not a
serious issue.

The authors also checked for \textbf{non-response bias} by comparing the
responses of people who answered early ($n = 185$) with those who answered
late after the reminder email ($n = 55$).
No significant differences were found between the two groups, which means
the results are unlikely to be distorted by who chose to respond.

\subsection{Advanced Model Validation}

The measurement model was validated using \textbf{Confirmatory Composite
Analysis (CCA)}.
The SRMR value was $0.034$, which is well below the commonly used threshold
of $0.08$, indicating a good overall model fit.

More importantly, the authors conducted a \textbf{robustness test} by
estimating an alternative model where DAC was assumed to cause BDU instead of
the other way around.
This reversed model performed worse on key fit indices---the SRMR was $0.050$
(compared to $0.034$ in the original) and the $d_{\text{ULS}}$ value of
$0.297$ exceeded its $95\%$ bootstrap threshold of $0.237$---confirming that
the original theoretical direction is the correct one.

\subsection{Mediation Analysis and Control Variables}

The study uses two separate methods to confirm the mediation effect:
a \textbf{Sobel test} and \textbf{bootstrapping} with $n = 4{,}999$ resamples.
The $95\%$ confidence interval for the indirect effect was $[0.123,\ 0.329]$,
which does not include zero, providing strong statistical support for the
mediation.

The authors also included \textbf{control variables}---firm size and IT
department size---to isolate the specific contribution of BDU and DAC.
Neither control variable had a significant effect on decision quality
(firm size: $\beta = 0.012$, $p > 0.05$; IT size: $\beta = 0.038$, $p > 0.05$),
which strengthens the interpretation of the main results.

\section{Weaknesses and Identified Errors}

\subsection{Discriminant Validity Problem (HTMT Ratio)}

One important statistical concern is related to the
\textbf{heterotrait-monotrait (HTMT) ratio}, which is a measure used to check
whether two constructs in a model are truly distinct from each other.
Table~6 of the article shows that the HTMT between BDU and DAC is $0.857$.

According to Henseler et al.\ (2015), who introduced this criterion and whose
paper has since become one of the most frequently cited works in the field of
structural equation modeling, values above the strict threshold of $0.85$ raise
concerns about discriminant validity.
The authors of the reviewed article acknowledge the $0.85$ threshold but prefer
to apply the more lenient threshold of $0.90$, under which their value would
still be acceptable.
However, using the stricter criterion, this result suggests that the survey
respondents may not have clearly separated the concept of ``using big data tools''
from the concept of ``having the capability to work with data.''
This is a noteworthy issue because the whole theoretical argument of the paper
depends on BDU and DAC being clearly different constructs.

\subsection{Reporting Inconsistencies in the Methodology}

The paper contains some clear clerical mistakes.
In the main text, the authors state that items BDU4, DQ1, DQ4, and DQ8
were removed from the analysis because their factor loadings were below the
acceptable threshold.
However, all four of these items are still listed in the Appendix, as if they
were part of the final measurement model.

Additionally, the text claims that ``three indicators were below the threshold
of $0.600$,'' but then proceeds to list four specific items.
These inconsistencies are not critical to the overall findings, but they do
reduce the overall precision of the paper's statistical reporting and suggest
that the manuscript was not thoroughly proofread before submission.

\subsection{Low Explanatory Power and Small Effect Size for H1}

The model explains only $32.5\%$ of the variance in decision-making quality
($R^{2} = 0.325$), which means that nearly $67.5\%$ of the factors that affect
decision quality are not included in this study.
While this level is not unusual in survey-based research, it does suggest that
the model is incomplete.

Furthermore, the \textbf{effect size} ($f^{2}$) for the direct relationship
between BDU and decision quality (H1) is only $0.084$.
This value is below the medium threshold of $0.15$ suggested by Cohen (1988).
This means that while the relationship is statistically significant, its
practical importance is relatively limited.
Adding other known predictors of decision quality, such as data quality or
top management support, could have made the model more powerful.

\subsection{Limited Generalizability and Self-Report Bias}

The sample consists entirely of agricultural firms located in China.
This means that the results may not apply to other types of organizations,
such as technology companies or healthcare providers, or to firms operating in
different cultural and regulatory environments.
The authors mention this limitation themselves, but do not suggest specific
future research contexts where replication would be most valuable.

In addition, all of the variables in the study---including ``decision-making
quality''---were measured through self-reported questionnaires filled out by
managers.
People who are asked to evaluate their own organization's performance may tend
to respond more positively than the actual situation warrants.
Using objective performance data, such as financial indicators or operational
efficiency metrics, as a supplement would have helped to reduce this
potential \textbf{optimism bias}.

\section*{References}

\begin{itemize}[leftmargin=1.5em, label={}, itemsep=0.4em]

  \item Cohen, J.\ (1988). \textit{Statistical power analysis for the behavioral
        sciences} (2nd ed.). Lawrence Erlbaum Associates.

  \item Henseler, J., Ringle, C.\ M., \& Sarstedt, M.\ (2015). A new criterion for
        assessing discriminant validity in variance-based structural equation
        modeling. \textit{Journal of the Academy of Marketing Science},
        \textit{43}(1), 115--135.
        \url{https://doi.org/10.1007/s11747-014-0403-8}

  \item Li, L., Lin, J., Ouyang, Y., \& Luo, X.\ R.\ (2022). Evaluating the impact
        of big data analytics usage on the decision-making quality of organizations.
        \textit{Technological Forecasting \& Social Change}, \textit{175}, 121355.
        \url{https://doi.org/10.1016/j.techfore.2021.121355}

  \item Lindell, M.\ K., \& Whitney, D.\ J.\ (2001). Accounting for common method
        variance in cross-sectional research designs. \textit{Journal of Applied
        Psychology}, \textit{86}(1), 114--121.
        \url{https://doi.org/10.1037/0021-9010.86.1.114}

\end{itemize}

\end{document}